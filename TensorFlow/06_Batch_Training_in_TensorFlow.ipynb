{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06. Batch Training in TensorFlow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNVTnVGeqXTE3/m6BPHtPPK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raj-vijay/dl/blob/master/06_Batch_Training_in_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-e1BjYefdjF",
        "colab_type": "text"
      },
      "source": [
        "**Batch Training**\n",
        "\n",
        "For large datasets, the entire dataset cannot be fit into the memory of a GPU. Thus, the dataset is split into batches for processing, and each batch is called an epoch. The process of splitting datasets into epochs for processing is called batch training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f172rCBp1mR3",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://raw.githubusercontent.com/raj-vijay/dl/master/images/Batch%20Training.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AJE027F2V28",
        "colab_type": "text"
      },
      "source": [
        "**King County Housing Dataset**\n",
        "\n",
        "Online property companies offer valuations of houses using machine learning techniques. The aim of this report is to predict the house sales in King County, Washington State, USA using Multiple Linear Regression (MLR). The dataset consisted of historic data of houses sold between May 2014 to May 2015. We will predict the sales of houses in King County with an accuracy of at least 75-80% and understand which factors are responsible for higher property value - $650K and above.”\n",
        "\n",
        "\n",
        "The dataset consists of house prices from King County an area in the US State of Washington, this data also covers Seattle. The dataset was obtained from Kaggle*. This data was published/released under CC0*: Public Domain. Unfortunately, the user has not indicated the source of the data. Please find the citation and database description in the Glossary and Bibliography. \n",
        "\n",
        "The dataset consisted of 21 variables and 21613 observations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64249CqWx5LQ",
        "colab_type": "text"
      },
      "source": [
        "Installing Kaggle Package to access the diabetes dataset from Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lh74UthzvMhN",
        "colab_type": "code",
        "outputId": "028ca778-0e5d-4a8c-cc95-e3dea366e988",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.38.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2019.11.28)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.21.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.8)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaBzyWRYx6ZH",
        "colab_type": "text"
      },
      "source": [
        "Make .kaggle directory under root to import the Kaggle Authentication JSON."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsMHWbO7x8eI",
        "colab_type": "code",
        "outputId": "20311b8b-a6f2-4793-f153-d22e7b888de0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!mkdir ~/.kaggle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAqlQYbryA_S",
        "colab_type": "text"
      },
      "source": [
        "Change file path to root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pz4SslxCyBxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4MViD2yyFMZ",
        "colab_type": "text"
      },
      "source": [
        "Chmod 600 (chmod a+rwx,u-x,g-rwx,o-rwx) sets permissions so that, (U)ser / owner can read, can write and can't execute. (G)roup can't read, can't write and can't execute. (O)thers can't read, can't write and can't execute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZy93l-pyHe_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gxO15ZnyKEQ",
        "colab_type": "text"
      },
      "source": [
        "Download housing dataset from Kaggle!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue7T1xCXyNbD",
        "colab_type": "code",
        "outputId": "989810f3-0843-4e7f-822f-ddd5a57d33cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!kaggle datasets download -d shivachandel/kc-house-data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kc-house-data.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p88VUprSyTu_",
        "colab_type": "text"
      },
      "source": [
        "**Load data using pandas**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BTACr2z2RYO",
        "colab_type": "text"
      },
      "source": [
        "- pd.read_csv() allows us to load data in batches\n",
        "- Avoid loading entire dataset\n",
        "- chunksize parameter provides batch size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiO6gKggyVET",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import pandas under the alias pd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assign the path to a string variable named data_path\n",
        "data_path = '/content/kc-house-data.zip'\n",
        "\n",
        "# Load data in batches\n",
        "for batch in pd.read_csv(data_path, compression='zip', chunksize=100):\n",
        "  # Extract price column\n",
        "  price = np.array(batch['price'], np.float32)\n",
        "  # Extract size column\n",
        "  size = np.array(batch['sqft_living'], np.float32)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0aukQkRy6SI",
        "colab_type": "code",
        "outputId": "73852d3b-ce5c-4c6e-bf39-e77a554b24bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Print the price column of housing\n",
        "print(price)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1537000.  467000.  224000.  507250.  429000.  610685. 1007500.  475000.\n",
            "  360000.  400000.  402101.  400000.  325000.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDJJfpQd3tCt",
        "colab_type": "text"
      },
      "source": [
        "**Training a linear model in batches**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgjRs6Wx3va2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import tensorflow, pandas, and numpy\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2wD33_o3yJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define trainable variables\n",
        "intercept = tf.Variable(0.1, tf.float32)\n",
        "slope = tf.Variable(0.1, tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z12bQqng30aq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the model\n",
        "def linear_regression(intercept, slope, features):\n",
        "  return intercept + features*slope"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J97hDEPk316j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute predicted values and return loss function\n",
        "def loss_function(intercept, slope, targets, features):\n",
        "  predictions = linear_regression(intercept, slope, features)\n",
        "  return tf.keras.losses.mse(targets, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZ-2JOzr38Iz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define optimization operation\n",
        "opt = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l00PuWCb3_wU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the data in batches from pandas\n",
        "for batch in pd.read_csv(data_path, compression='zip', chunksize=100):\n",
        "  # Extract the target and feature columns\n",
        "  price_batch = np.array(batch['price'], np.float32)\n",
        "  size_batch = np.array(batch['sqft_living'], np.float32)\n",
        "  # Minimize the loss function\n",
        "  opt.minimize(lambda: loss_function(intercept, slope, price_batch, size_batch),\n",
        "  var_list=[intercept, slope])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QrZNpCF4SAK",
        "colab_type": "code",
        "outputId": "3e1526e7-501a-402d-f712-8f322bcab6c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Print parameter values\n",
        "print(intercept.numpy(), slope.numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.31799173 0.31615734\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyiO65mL4aaI",
        "colab_type": "text"
      },
      "source": [
        "**Full Sample**\n",
        "1. One update per epoch\n",
        "2. Accepts dataset without modification\n",
        "3. Limited by memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5IY_WfE4meS",
        "colab_type": "text"
      },
      "source": [
        "**Batch Training**\n",
        "1. Multiple updates per epoch\n",
        "2. Requires division of dataset\n",
        "3. No limit on dataset size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A93LeV3q6JrS",
        "colab_type": "text"
      },
      "source": [
        "**Preparing to batch train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37bhQ5Rc6XHR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import Variable, float32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4xr35ui6Kvb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the intercept and slope\n",
        "intercept = Variable(10.0, float32)\n",
        "slope = Variable(0.5, float32)\n",
        "\n",
        "# Define the model\n",
        "def linear_regression(intercept, slope, features):\n",
        "\t# Define the predicted values\n",
        "\treturn intercept + slope*features\n",
        "\n",
        "# Define the loss function\n",
        "def loss_function(intercept, slope, targets, features):\n",
        "    # Define the predicted values\n",
        "    predictions = linear_regression(intercept, slope, features)\n",
        "    # Define the MSE loss\n",
        "    return keras.losses.mse(targets, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGuDjZ-u8vAJ",
        "colab_type": "code",
        "outputId": "31732379-32d2-423d-f7b6-5fd62000f899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Initialize adam optimizer\n",
        "opt = keras.optimizers.Adam()\n",
        "\n",
        "# Load data in batches\n",
        "for batch in pd.read_csv(data_path, compression='zip', chunksize=100):\n",
        "\tsize_batch = np.array(batch['sqft_living'], np.float32)\n",
        "\n",
        "\t# Extract the price values for the current batch\n",
        "\tprice_batch = np.array(batch['price'], np.float32)\n",
        "\n",
        "\t# Complete the loss, fill in the variable list, and minimize\n",
        "\topt.minimize(lambda: loss_function(intercept, slope, price_batch, size_batch), var_list=[intercept, slope])\n",
        "\n",
        "# Print trained parameters\n",
        "print(intercept.numpy(), slope.numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10.217994 0.7161536\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
